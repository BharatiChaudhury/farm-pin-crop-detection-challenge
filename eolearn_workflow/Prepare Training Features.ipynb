{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from sentinelhub import CRS, BBoxSplitter, BBox\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from eolearn.io.local_io import *\n",
    "from contextlib import contextmanager  \n",
    "\n",
    "import rasterio\n",
    "from rasterio import Affine, MemoryFile\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "\n",
    "# Imports from eo-learn and sentinelhub-py\n",
    "from eolearn.core import EOTask, EOPatch, LinearWorkflow, FeatureType, OverwritePermission, \\\n",
    "    LoadFromDisk, SaveToDisk, EOExecutor\n",
    "from eolearn.io import S2L1CWCSInput, ExportToTiff\n",
    "from eolearn.mask import AddCloudMaskTask, get_s2_pixel_cloud_detector, AddValidDataMaskTask\n",
    "from eolearn.geometry import VectorToRaster, PointSamplingTask, ErosionTask\n",
    "from eolearn.features import LinearInterpolation, SimpleFilterTask, InterpolationTask, ValueFilloutTask, \\\n",
    "    HaralickTask\n",
    "from sentinelhub import BBoxSplitter, BBox, CRS, CustomUrlParam\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [[0,1,4],\n",
    "               [3,7],\n",
    "               [2,6], \n",
    "                 [5,9,11],\n",
    "                   [8,10]]\n",
    "\n",
    "places  = [[1,  2,  3],\n",
    "                   [9 , 10],\n",
    "                   [15, 16], \n",
    "                       [22 , 23, 24],\n",
    "                            [29, 30]]\n",
    "\n",
    "patchIDs = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "path_out = './eopatches-large/'\n",
    "\n",
    "data_path = Path('../data')\n",
    "sentinel_path = data_path/'sentinel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenateData(EOTask):\n",
    "    \"\"\" Task to concatenate data arrays along the last dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_name, feature_names_to_concatenate):\n",
    "        self.feature_name = feature_name\n",
    "        self.feature_names_to_concatenate = feature_names_to_concatenate\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        arrays = [eopatch.data[name] for name in self.feature_names_to_concatenate]\n",
    "\n",
    "        eopatch.add_feature(FeatureType.DATA, self.feature_name, \n",
    "                            np.concatenate(arrays, axis=-1))\n",
    "\n",
    "        return eopatch\n",
    "    \n",
    "    \n",
    "    \n",
    "class ValidDataFractionPredicate:\n",
    "    \"\"\" Predicate that defines if a frame from EOPatch's time-series is valid or not. \n",
    "    Frame is valid, if the \n",
    "    valid data fraction is above the specified threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def __call__(self, array):\n",
    "        coverage = np.sum(array.astype(np.uint8)) / np.prod(array.shape)\n",
    "        return coverage > self.threshold\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "# TASK TO LOAD EXISTING EOPATCHES\n",
    "load = LoadFromDisk(path_out)\n",
    "\n",
    "# TASK FOR CONCATENATION\n",
    "concatenate = ConcatenateData('FEATURES', ['BANDS', 'NDVI', 'NDWI', 'NORM'])\n",
    "\n",
    "# TASK FOR FILTERING OUT TOO CLOUDY SCENES\n",
    "# keep frames with > 80 % valid coverage\n",
    "valid_data_predicate = ValidDataFractionPredicate(0.8)\n",
    "filter_task = SimpleFilterTask((FeatureType.MASK, 'IS_VALID'), valid_data_predicate)\n",
    "\n",
    "# TASK FOR LINEAR INTERPOLATION\n",
    "# linear interpolation of full time-series and date resampling\n",
    "\n",
    "resampled_range = ('2017-01-01', '2017-8-20', 29)\n",
    "linear_interp = LinearInterpolation(\n",
    "    'FEATURES', # name of field to interpolate\n",
    "    mask_feature=(FeatureType.MASK, 'IS_VALID'), # mask to be used in interpolation\n",
    "    copy_features=[(FeatureType.MASK_TIMELESS, 'LULC'),\n",
    "                   (FeatureType.MASK_TIMELESS, 'TEST_FIELD_ID'),\n",
    "                  (FeatureType.MASK_TIMELESS, 'TRAIN_FIELD_ID')], # features to keep\\\n",
    "    resample_range=resampled_range, # set the resampling range\n",
    "    bounds_error=False # extrapolate with NaN's\n",
    ")\n",
    "\n",
    "# TASK FOR EROSION\n",
    "# erode each class of the reference map\n",
    "erosion = ErosionTask(mask_feature=(FeatureType.MASK_TIMELESS,'LULC','LULC_ERODED'), disk_radius=1)\n",
    "\n",
    "# TASK FOR SPATIAL SAMPLING\n",
    "# Uniformly sample about pixels from patches\n",
    "n_samples = int(1e5) # no. of pixels to sample\n",
    "ref_labels = [1,2,3,4,5,6,7,8,9] # reference labels to take into account when sampling\n",
    "\n",
    "\n",
    "copy_extrapolate = ValueFilloutTask(feature = 'FEATURES', operations='fb', value=np.nan, axis=0)\n",
    "\n",
    "\n",
    "# spatial_sampling = PointSamplingTask(\n",
    "#     n_samples=n_samples, \n",
    "#     ref_mask_feature='LULC_ERODED', \n",
    "#     ref_labels=ref_labels, \n",
    "#     sample_features=[  # tag fields to sample\n",
    "#         (FeatureType.DATA, 'FEATURES'),\n",
    "#         (FeatureType.MASK_TIMELESS, 'LULC_ERODED')\n",
    "#     ])\n",
    "\n",
    "path_out_sampled = './eopatches_large_with_ids/'\n",
    "if not os.path.isdir(path_out_sampled):\n",
    "    os.makedirs(path_out_sampled)\n",
    "save = SaveToDisk(path_out_sampled, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "land_cover_path = data_path/'test'\n",
    "\n",
    "land_cover_data = gpd.read_file(land_cover_path)\n",
    "\n",
    "land_cover_data.to_crs({'init': 'EPSG:32734'},inplace=True)\n",
    "\n",
    "land_cover_data['Field_Id'] = land_cover_data.Field_Id.astype(np.uint16)\n",
    "\n",
    "test_field_id_raster = VectorToRaster(land_cover_data, (FeatureType.MASK_TIMELESS, 'TEST_FIELD_ID'),\n",
    "                                    values_column='Field_Id', raster_shape=(FeatureType.MASK, 'IS_VALID'),\n",
    "                                    raster_dtype=np.uint16)\n",
    "\n",
    "land_cover_path = data_path/'train'\n",
    "\n",
    "land_cover_data = gpd.read_file(land_cover_path)\n",
    "\n",
    "land_cover_data.dropna(inplace=True)\n",
    "\n",
    "land_cover_data.to_crs({'init': 'EPSG:32734'},inplace=True)\n",
    "\n",
    "land_cover_data['Field_Id'] = land_cover_data.Field_Id.astype(np.uint16)\n",
    "\n",
    "train_field_id_raster = VectorToRaster(land_cover_data, (FeatureType.MASK_TIMELESS, 'TRAIN_FIELD_ID'),\n",
    "                                    values_column='Field_Id', raster_shape=(FeatureType.MASK, 'IS_VALID'),\n",
    "                                    raster_dtype=np.uint16)\n",
    "\n",
    "# Define the workflow\n",
    "workflow = LinearWorkflow(\n",
    "    load,\n",
    "    test_field_id_raster,\n",
    "    train_field_id_raster,\n",
    "    concatenate,\n",
    "    filter_task,\n",
    "    linear_interp,\n",
    "    copy_extrapolate,\n",
    "    #erosion,\n",
    "    #spatial_sampling,\n",
    "    save\n",
    ")\n",
    "\n",
    "%%time\n",
    "   \n",
    "execution_args = []\n",
    "for idx in range(len(patchIDs)):\n",
    "    execution_args.append({\n",
    "        load: {'eopatch_folder': 'eopatch_{}'.format(idx)},\n",
    "        save: {'eopatch_folder': 'eopatch_{}'.format(idx)}\n",
    "    })\n",
    "    \n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "executor.run(workers=5, multiprocess=True)\n",
    "\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [[0,1,4],\n",
    "               [3,7],\n",
    "               [2,6], \n",
    "                 [5,9,11],\n",
    "                   [8,10]]\n",
    "\n",
    "places  = [[1,  2,  3],\n",
    "                   [9 , 10],\n",
    "                   [15, 16], \n",
    "                       [22 , 23, 24],\n",
    "                            [29, 30]]\n",
    "\n",
    "patchIDs = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "path_out = './eopatches-large/'\n",
    "\n",
    "data_path = Path('../data')\n",
    "sentinel_path = data_path/'sentinel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenateData(EOTask):\n",
    "    \"\"\" Task to concatenate data arrays along the last dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_name, feature_names_to_concatenate):\n",
    "        self.feature_name = feature_name\n",
    "        self.feature_names_to_concatenate = feature_names_to_concatenate\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        arrays = [eopatch.data[name] for name in self.feature_names_to_concatenate]\n",
    "\n",
    "        eopatch.add_feature(FeatureType.DATA, self.feature_name, \n",
    "                            np.concatenate(arrays, axis=-1))\n",
    "\n",
    "        return eopatch\n",
    "    \n",
    "    \n",
    "    \n",
    "class ValidDataFractionPredicate:\n",
    "    \"\"\" Predicate that defines if a frame from EOPatch's time-series is valid or not. \n",
    "    Frame is valid, if the \n",
    "    valid data fraction is above the specified threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def __call__(self, array):\n",
    "        coverage = np.sum(array.astype(np.uint8)) / np.prod(array.shape)\n",
    "        return coverage > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK TO LOAD EXISTING EOPATCHES\n",
    "load = LoadFromDisk(path_out)\n",
    "\n",
    "# TASK FOR CONCATENATION\n",
    "concatenate = ConcatenateData('FEATURES', ['BANDS', 'NDVI', 'NDWI', 'NORM'])\n",
    "\n",
    "# TASK FOR FILTERING OUT TOO CLOUDY SCENES\n",
    "# keep frames with > 80 % valid coverage\n",
    "valid_data_predicate = ValidDataFractionPredicate(0.8)\n",
    "filter_task = SimpleFilterTask((FeatureType.MASK, 'IS_VALID'), valid_data_predicate)\n",
    "\n",
    "# TASK FOR LINEAR INTERPOLATION\n",
    "# linear interpolation of full time-series and date resampling\n",
    "\n",
    "resampled_range = ('2017-01-01', '2017-8-20', 29)\n",
    "linear_interp = LinearInterpolation(\n",
    "    'FEATURES', # name of field to interpolate\n",
    "    mask_feature=(FeatureType.MASK, 'IS_VALID'), # mask to be used in interpolation\n",
    "    copy_features=[(FeatureType.MASK_TIMELESS, 'LULC'),\n",
    "                   (FeatureType.MASK_TIMELESS, 'TEST_FIELD_ID'),\n",
    "                  (FeatureType.MASK_TIMELESS, 'TRAIN_FIELD_ID')], # features to keep\\\n",
    "    resample_range=resampled_range, # set the resampling range\n",
    "    bounds_error=False # extrapolate with NaN's\n",
    ")\n",
    "\n",
    "# TASK FOR EROSION\n",
    "# erode each class of the reference map\n",
    "erosion = ErosionTask(mask_feature=(FeatureType.MASK_TIMELESS,'LULC','LULC_ERODED'), disk_radius=1)\n",
    "\n",
    "copy_extrapolate = CopyExtrapolate(name = 'FEATURES')\n",
    "\n",
    "texture_features = HaralickTask(f, AVAILABLE_TEXTURES_SKIMAGE)\n",
    "\n",
    "path_out_sampled = './eopatches_large_with_ids/'\n",
    "if not os.path.isdir(path_out_sampled):\n",
    "    os.makedirs(path_out_sampled)\n",
    "save = SaveToDisk(path_out_sampled, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_cover_path = data_path/'test'\n",
    "\n",
    "land_cover_data = gpd.read_file(land_cover_path)\n",
    "\n",
    "land_cover_data.to_crs({'init': 'EPSG:32734'},inplace=True)\n",
    "\n",
    "land_cover_data['Field_Id'] = land_cover_data.Field_Id.astype(np.uint16)\n",
    "\n",
    "test_field_id_raster = VectorToRaster(land_cover_data, (FeatureType.MASK_TIMELESS, 'TEST_FIELD_ID'),\n",
    "                                    values_column='Field_Id', raster_shape=(FeatureType.MASK, 'IS_VALID'),\n",
    "                                    raster_dtype=np.uint16)\n",
    "\n",
    "land_cover_path = data_path/'train'\n",
    "\n",
    "land_cover_data = gpd.read_file(land_cover_path)\n",
    "\n",
    "land_cover_data.dropna(inplace=True)\n",
    "\n",
    "land_cover_data.to_crs({'init': 'EPSG:32734'},inplace=True)\n",
    "\n",
    "land_cover_data['Field_Id'] = land_cover_data.Field_Id.astype(np.uint16)\n",
    "\n",
    "train_field_id_raster = VectorToRaster(land_cover_data, (FeatureType.MASK_TIMELESS, 'TRAIN_FIELD_ID'),\n",
    "                                    values_column='Field_Id', raster_shape=(FeatureType.MASK, 'IS_VALID'),\n",
    "                                    raster_dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "workflow = LinearWorkflow(\n",
    "    load,\n",
    "    test_field_id_raster,\n",
    "    train_field_id_raster,\n",
    "    concatenate,\n",
    "    filter_task,\n",
    "    linear_interp,\n",
    "    copy_extrapolate,\n",
    "    erosion,\n",
    "    \n",
    "    save\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8279e1884045c2b0a96ff7122772ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1.05 s, sys: 115 ms, total: 1.16 s\n",
      "Wall time: 11min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "   \n",
    "execution_args = []\n",
    "for idx in range(len(patchIDs)):\n",
    "    execution_args.append({\n",
    "        load: {'eopatch_folder': 'eopatch_{}'.format(idx)},\n",
    "        save: {'eopatch_folder': 'eopatch_{}'.format(idx)}\n",
    "    })\n",
    "    \n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "executor.run(workers=5, multiprocess=True)\n",
    "\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample training patchlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(patch_path, patch_idx, out_path, size=64, sample_factor=25):\n",
    "    \n",
    "    patch_path = Path(patch_path)\n",
    "    out_path = Path(out_path)\n",
    "    \n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    patch = EOPatch.load(patch_path/f'eopatch_{patch_idx}')\n",
    "    \n",
    "    half_size = size//2\n",
    "    \n",
    "    targets = patch.mask_timeless['LULC']\n",
    "    \n",
    "    patch_shape = targets.shape\n",
    "    \n",
    "    shape_x = patch_shape[0]\n",
    "    shape_y = patch_shape[1]\n",
    "    \n",
    "    features = patch.data['FEATURES'][...,[3,2,1,8,-3,-1]] # R G B NIR NDVI NORM\n",
    "    \n",
    "    features[7][np.isnan(features[7])] = features[6][np.isnan(features[7])]\n",
    "    \n",
    "    non_empty_targets = np.indices(patch_shape)[:,targets!=0]\n",
    "    \n",
    "    target_xs = non_empty_targets[0]\n",
    "    target_ys = non_empty_targets[1]\n",
    "\n",
    "    target_count = len(target_xs)\n",
    "    \n",
    "    samples = (target_count*sample_factor)//(size**2)\n",
    "    \n",
    "    #print(f'saving {samples} patchlets from patch {patch_idx}')\n",
    "\n",
    "    for i in range(samples):\n",
    "        index = np.random.randint(target_count)\n",
    "        \n",
    "        x_offset = np.clip(target_xs[index], half_size, shape_x-half_size) # ensure sample won't exceed patch\n",
    "        y_offset = np.clip(target_ys[index], half_size, shape_y-half_size) # ensure sample won't exceed patch\n",
    "        \n",
    "        targ_arr = targets [x_offset-half_size:x_offset+half_size, y_offset-half_size:y_offset+half_size,0]\n",
    "        feat_arr = features[:,x_offset-half_size:x_offset+half_size, y_offset-half_size:y_offset+half_size,:]\n",
    "        \n",
    "        targ_fn = f'patch_{patch_idx}_x_{x_offset}_y_{y_offset}_targ.pkl'\n",
    "        feat_fn = f'patch_{patch_idx}_x_{x_offset}_y_{y_offset}_feat.pkl'\n",
    "        \n",
    "        pkl.dump(targ_arr, open(out_path/targ_fn,'wb'))\n",
    "        pkl.dump(feat_arr, open(out_path/feat_fn,'wb'))\n",
    "    \n",
    "    del patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchlet_path = Path('./training_patchlets')\n",
    "patch_path = Path('./eopatches_large_with_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for patch_idx in tqdm(range(12)):\n",
    "#     save_samples(patch_path=patch_path, \n",
    "#                  patch_idx=patch_idx, \n",
    "#                  out_path=patchlet_path, \n",
    "#                  size=64, \n",
    "#                  sample_factor=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([str(f) for f in patchlet_path.glob('*_targ.pkl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "targs = [str(f) for f in patchlet_path.glob('patch_10_*_targ.pkl')][:9]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "for i,f in enumerate(targs):\n",
    "    ax = plt.subplot(3, 3, i+1)\n",
    "    targ = pkl.load(open(f,'rb'))\n",
    "    plt.imshow(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Usually we use `%matplotlib inline`. However we need `notebook` for the anim to render in the notebook.\n",
    "%matplotlib notebook\n",
    "\n",
    "fps = 1\n",
    "nSeconds = 8\n",
    "\n",
    "fig, axes = plt.subplots(3,3,sharey=True,sharex=True, figsize=(10,10))\n",
    "\n",
    "feats = [str(f).replace('targ','feat') for f in targs]\n",
    "\n",
    "datas = [[pkl.load(open(feat,'rb'))[i,...,-2] for i in range(8)] for feat in feats]\n",
    "\n",
    "imgs = [axes[j//3,j%3].imshow(datas[j][0], interpolation='none', aspect='auto', \n",
    "                              vmin=0, vmax=0.8, cmap=plt.get_cmap('YlGn'))\n",
    "      for j in range(len(feats))]\n",
    "\n",
    "cb = fig.colorbar(imgs[0], ax=axes.ravel().tolist(), orientation='horizontal', pad=0.01, aspect=100)\n",
    "cb.ax.tick_params(labelsize=20)\n",
    "\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August']\n",
    "def animate_func(i):\n",
    "    fig.suptitle(f'NDVI: {months[i]}')\n",
    "    for j in range(len(feats)):\n",
    "        imgs[j].set_array(datas[j][i])\n",
    "    return [imgs]\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "                               fig, \n",
    "                               animate_func, \n",
    "                               frames = nSeconds * fps,\n",
    "                               interval = 1000 / fps, # in ms\n",
    "                               )\n",
    "\n",
    "anim.save('nvdi.gif', writer=mpl.animation.PillowWriter(fps=fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Usually we use `%matplotlib inline`. However we need `notebook` for the anim to render in the notebook.\n",
    "%matplotlib notebook\n",
    "\n",
    "fps = 1\n",
    "nSeconds = 8\n",
    "\n",
    "fig, axes = plt.subplots(3,3,sharey=True,sharex=True, figsize=(10,10))\n",
    "\n",
    "feats = [str(f).replace('targ','feat') for f in targs]\n",
    "\n",
    "datas = [[np.clip(pkl.load(open(feat,'rb'))[i,...,0:3]*3.5,0,1) for i in range(8)] for feat in feats]\n",
    "\n",
    "imgs = [axes[j//3,j%3].imshow(datas[j][0], interpolation='none', aspect='auto', cmap=plt.get_cmap('YlGn'), vmin=0, vmax=1) \n",
    "      for j in range(len(feats))]\n",
    "\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August']\n",
    "def animate_func(i):\n",
    "    fig.suptitle(f'True colour: {months[i]}')\n",
    "    for j in range(len(feats)):\n",
    "        imgs[j].set_array(datas[j][i])\n",
    "    return [imgs]\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "                               fig, \n",
    "                               animate_func, \n",
    "                               frames = nSeconds * fps,\n",
    "                               interval = 1000 / fps, # in ms\n",
    "                               )\n",
    "\n",
    "anim.save('nvdi.gif', writer=mpl.animation.PillowWriter(fps=fps))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patchlet_path = Path('./test_patchlets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_grid(patch_path, patch_idx, out_path, size=64):\n",
    "    \n",
    "    patch_path = Path(patch_path)\n",
    "    out_path = Path(out_path)\n",
    "    \n",
    "    out_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    patch = EOPatch.load(patch_path/f'eopatch_{patch_idx}')\n",
    "    \n",
    "    half_size = size//2\n",
    "    \n",
    "    test_field_ids = patch.mask_timeless['TEST_FIELD_ID']\n",
    "    \n",
    "    patch_shape = test_field_ids.squeeze().shape\n",
    "    \n",
    "    shape_x = patch_shape[0]\n",
    "    shape_y = patch_shape[1]\n",
    "    \n",
    "    features = patch.data['FEATURES'][...,[3,2,1,8,-3,-1]] # R G B NIR NDVI NORM\n",
    "\n",
    "    # extrapolate final timepoint clouds if required\n",
    "    features[7][np.isnan(features[7])] = features[6][np.isnan(features[7])] \n",
    "\n",
    "    c = (shape_x//size)+1\n",
    "    r = (shape_y//size)+1\n",
    "\n",
    "    for i in range(c):\n",
    "        for j in range(r):\n",
    "            x_offset = i*64\n",
    "            y_offset = j*64\n",
    "            \n",
    "            field_id_fn = f'patch_{patch_idx}_x_{x_offset}_y_{y_offset}_targ.pkl'\n",
    "            feat_fn     = f'patch_{patch_idx}_x_{x_offset}_y_{y_offset}_feat.pkl'\n",
    "            \n",
    "            patchlet_field_ids = test_field_ids[x_offset:x_offset+size, \n",
    "                                                y_offset:y_offset+size,...]\n",
    "        \n",
    "            if patchlet_field_ids.max() > 0:\n",
    "                \n",
    "                feat_arr = features[:,x_offset:x_offset+size, \n",
    "                                      y_offset:y_offset+size,:]\n",
    "                \n",
    "                if (patchlet_field_ids.shape[0] < size) or (patchlet_field_ids.shape[1] < size):\n",
    "                    short_x = size - patchlet_field_ids.shape[0]\n",
    "                    short_y = size - patchlet_field_ids.shape[1]\n",
    "                    \n",
    "                    patchlet_field_ids = np.pad(patchlet_field_ids, \n",
    "                                                   ((0,short_x),(0,short_y),(0,0)), \n",
    "                                                   mode='constant', \n",
    "                                                   constant_values=0)\n",
    "\n",
    "                    feat_arr           = np.pad(feat_arr, \n",
    "                                                   ((0,0),(0,short_x),(0,short_y),(0,0)), \n",
    "                                                   mode='reflect')\n",
    "                pkl.dump(patchlet_field_ids, open(out_path/field_id_fn,'wb'))\n",
    "                pkl.dump(feat_arr, open(out_path/feat_fn,'wb'))\n",
    "    del patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(12)):\n",
    "    save_test_grid(patch_path=patch_path, patch_idx=i, out_path=test_patchlet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls test_patchlets | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_test_grid(patch_idx, patchlet_path, shape_x=1335, shape_y=1353, size=64):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    \n",
    "    patchlet_path = Path(patchlet_path)\n",
    "    \n",
    "    c = (shape_x//size)+1\n",
    "    r = (shape_y//size)+1\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            x_offset = i*64\n",
    "            y_offset = j*64\n",
    "            \n",
    "            ax = plt.subplot(r,c, i*c + j+1)\n",
    "\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_aspect(\"auto\")\n",
    "            \n",
    "            field_id_fn = Path(patchlet_path/f'patch_{patch_idx}_x_{x_offset}_y_{y_offset}_targ.pkl')\n",
    "\n",
    "            if field_id_fn.exists():\n",
    "                patchlet = pkl.load(open(field_id_fn,'rb'))\n",
    "                plt.imshow(patchlet.squeeze(),vmin=0,vmax=3600)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_idx=6\n",
    "\n",
    "test_field_ids = EOPatch.load(patch_path/f'eopatch_{patch_idx}').mask_timeless['TEST_FIELD_ID'].squeeze()\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.imshow(test_field_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_test_grid(patch_idx=patch_idx, patchlet_path=test_patchlet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
